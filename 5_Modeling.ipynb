{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4086610b",
   "metadata": {},
   "source": [
    "### 1. Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8576c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd731e0",
   "metadata": {},
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for loading the data\n",
    "train_data_path = 'train_data.csv'\n",
    "test_data_path = 'test_data.csv'\n",
    "train_target_path = 'train_target.csv'\n",
    "test_target_path = 'test_target.csv'\n",
    "\n",
    "# Load training and testing features from CSV files\n",
    "X_train = pd.read_csv(train_data_path)\n",
    "X_test = pd.read_csv(test_data_path)\n",
    "\n",
    "# Load training and testing target variables from CSV files\n",
    "y_train = pd.read_csv(train_target_path)['WQI']\n",
    "y_test = pd.read_csv(test_target_path)['WQI'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f192b2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the summary information of the loaded data\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c3b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794a62c",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a909d",
   "metadata": {},
   "source": [
    "Based on the information provided by info(), it seems that both the training and testing datasets have been loaded successfully without any missing values. Each dataset contains 10 columns, all of which are numerical (float64), including the target variable 'WQI'. There are 820 samples in the training set and 205 samples in the testing set, with 10 features each. \n",
    "\n",
    "Since the target variable, WQI, is continuous, Linear Regression, Random Forest Regression, and XGBoost Regression model are used to explore different approaches and leverage the strengths of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fd5ad",
   "metadata": {},
   "source": [
    "#### 3a. Making a Linear Regression Model: first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05589879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "mse_score_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"R-squared (R2) on Testing Set:\", r2_linear)\n",
    "print(\"Mean Squared Error (MSE) on Testing Set:\", mse_score_linear)\n",
    "print(\"Mean Absolute Error (MAE) on Testing Set:\", mae_linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the range of number of splits\n",
    "num_splits_range = [3, 5, 7, 10]\n",
    "\n",
    "# Dictionary to store cross-validation scores for each number of splits\n",
    "cv_scores_linear_test = {}\n",
    "cv_scores_linear_train = {}\n",
    "\n",
    "for num_splits in num_splits_range:\n",
    "    # Define cross-validation strategy with current number of splits\n",
    "    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation for Linear Regression\n",
    "    cv_scores_linear_test[num_splits] = cross_val_score(linear_model, X_test, y_test, cv=kf, scoring='r2')\n",
    "    cv_scores_linear_train[num_splits] = cross_val_score(linear_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "  \n",
    "    # Print the cross-validation scores for each number of splits\n",
    "    print(f\"{num_splits}-fold Cross-Validation Scores for Linear Regression:\")\n",
    "    print(\"Mean cross-validation score on testing set:\", np.mean(cv_scores_linear_test[num_splits]))\n",
    "    print(\"Mean cross-validation score on training set:\", np.mean(cv_scores_linear_train[num_splits]))\n",
    "    print(\"Standard deviation in cross-validation scores on testing set:\", np.std(cv_scores_linear_test[num_splits]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72873c16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the predictions against the actual values for Linear Regression\n",
    "plt.scatter(y_test, y_pred_linear)\n",
    "plt.plot(y_test, y_test, color='red', linestyle='--')  # Plot the perfect fit line\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Predictions vs Actual Values (Linear Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lasso model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the Lasso model\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "lasso_r2 = r2_score(y_test, y_pred_lasso)\n",
    "lasso_mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "lasso_mae = mean_absolute_error(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"Lasso Regression:\")\n",
    "print(\"R-squared (R2) on Testing Set:\", lasso_r2)\n",
    "print(\"Mean Squared Error (MSE) on Testing Set:\", lasso_mse)\n",
    "print(\"Mean Absolute Error (MAE) on Testing Set:\", lasso_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0af630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ridge model\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "\n",
    "# Fit the Ridge model to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the Ridge model\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Ridge model\n",
    "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "ridge_mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "print(\"R-squared (R2) on Testing Set:\", ridge_r2)\n",
    "print(\"Mean Squared Error (MSE) on Testing Set:\", ridge_mse)\n",
    "print(\"Mean Absolute Error (MAE) on Testing Set:\", ridge_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1cd8bc",
   "metadata": {},
   "source": [
    "#### 3b. Making a Random Forest Regression Methods: second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest Regression model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mse_score_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Regression:\")\n",
    "print(\"R-squared (R2) on Testing Set:\", r2_rf)\n",
    "print(\"Mean Squared Error (MSE) on Testing Set:\", mse_score_rf)\n",
    "print(\"Mean Absolute Error (MAE) on Testing Set:\", mae_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store cross-validation scores for each number of splits\n",
    "cv_scores_rf_test = {}\n",
    "cv_scores_rf_train = {}\n",
    "\n",
    "# Define the Random Forest Regression model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "for num_splits in num_splits_range:\n",
    "    # Define cross-validation strategy with current number of splits\n",
    "    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=kf, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the best model using cross-validation on the training set\n",
    "    cv_scores_rf_train[num_splits] = cross_val_score(best_rf_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "    \n",
    "    # Evaluate the best model using cross-validation on the testing set\n",
    "    cv_scores_rf_test[num_splits] = cross_val_score(best_rf_model, X_test, y_test, cv=kf, scoring='r2')\n",
    "\n",
    "# Print the cross-validation scores for each number of splits\n",
    "for num_splits in num_splits_range:\n",
    "    print(f\"{num_splits}-fold Cross-Validation Scores:\")\n",
    "    print(\"Mean cross-validation score on training set:\", np.mean(cv_scores_rf_train[num_splits]))\n",
    "    print(\"Mean cross-validation score on testing set:\", np.mean(cv_scores_rf_test[num_splits]))\n",
    "    print(\"Standard deviation in cross-validation scores on testing set:\", np.std(cv_scores_rf_test[num_splits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions against the actual values\n",
    "plt.scatter(y_test, y_pred_rf, color='blue', label='Predictions')\n",
    "plt.plot(y_test, y_test, color='red', linestyle='--', label='Perfect Fit')\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Predictions vs Actual Values for Random Forest Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7f259",
   "metadata": {},
   "source": [
    "#### 3d. Making a XGBoost Regression Model: third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest Regression model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mse_score_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Regression:\")\n",
    "print(\"R-squared (R2) on Testing Set:\", r2_xgb)\n",
    "print(\"Mean Squared Error (MSE) on Testing Set:\", mse_score_xgb)\n",
    "print(\"Mean Absolute Error (MAE) on Testing Set:\", mae_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb519be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store cross-validation scores for each number of splits\n",
    "cv_scores_xgb_test = {}\n",
    "cv_scores_xgb_train = {}\n",
    "\n",
    "# Define the XGBoost Regression model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "for num_splits in num_splits_range:\n",
    "    # Define cross-validation strategy with current number of splits\n",
    "    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform hyperparameter tuning with GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'min_child_weight': [1, 2, 3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=kf, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the best model using cross-validation on the training set\n",
    "    cv_scores_xgb_train[num_splits] = cross_val_score(best_xgb_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "    \n",
    "    # Evaluate the best model using cross-validation on the testing set\n",
    "    cv_scores_xgb_test[num_splits] = cross_val_score(best_xgb_model, X_test, y_test, cv=kf, scoring='r2')\n",
    "\n",
    "# Print the cross-validation scores for each number of splits\n",
    "for num_splits in num_splits_range:\n",
    "    print(f\"{num_splits}-fold Cross-Validation Scores:\")\n",
    "    print(\"Mean cross-validation score on training set:\", np.mean(cv_scores_xgb_train[num_splits]))\n",
    "    print(\"Mean cross-validation score on testing set:\", np.mean(cv_scores_xgb_test[num_splits]))\n",
    "    print(\"Standard deviation in cross-validation scores on testing set:\", np.std(cv_scores_xgb_test[num_splits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions against the actual values\n",
    "plt.scatter(y_test, y_pred_xgb, color='blue', label='Predictions')\n",
    "plt.plot(y_test, y_test, color='red', linestyle='--', label='Perfect Fit')\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Predictions vs Actual Values for XGBoost Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41980523",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bb56c",
   "metadata": {},
   "source": [
    "### 5. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a3240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
